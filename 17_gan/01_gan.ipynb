{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bf84b24-0679-498f-b663-2bd249c9d929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b4d9ca-d071-47b0-a8a3-9c065f3ac40d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d7f6e7-22b2-47ce-a24b-5ee176e331cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_generator_network(hidden_layers=1, hidden_units=100, output_units=784):\n",
    "    model = tf.keras.Sequential()\n",
    "    for i in range(hidden_layers):\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(units=hidden_units, use_bias=False))\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    " \n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(units=output_units, activation='tanh'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_discriminator_network(hidden_layers=1, hidden_units=100, output_units=1):\n",
    "    model = tf.keras.Sequential()\n",
    "    for i in range(hidden_layers):\n",
    "        model.add(\n",
    "            tf.keras.layers.Dense(units=hidden_units))\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "        model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "        \n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(units=output_units,\n",
    "                              activation=None))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5060567-600c-413b-869b-7d959a443b82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [2. 3.]\n",
      " [4. 5.]\n",
      " [6. 7.]\n",
      " [8. 9.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[ 0.,  0.],\n",
       "       [ 4.,  6.],\n",
       "       [ 0., 10.],\n",
       "       [ 0., 14.],\n",
       "       [16.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "layer = tf.keras.layers.Dropout(.5, input_shape=(2,))\n",
    "data = np.arange(10).reshape(5, 2).astype(np.float32)\n",
    "print(data)\n",
    "\n",
    "outputs = layer(data, training=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aef98d5-e8da-4fc3-b142-a912f16115e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod([28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0e42f5a-db7d-4fbf-8379-acd5843a142c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               2000      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 784)               79184     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,184\n",
      "Trainable params: 81,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_size = (28, 28)\n",
    "z_size = 20\n",
    "mode_z = 'uniform'\n",
    "gen_hidden_layers = 1\n",
    "gen_hidden_size = 100\n",
    "disc_hidden_layers = 1\n",
    "disc_hidden_size = 100\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "gen_model = make_generator_network(hidden_layers=gen_hidden_layers,\n",
    "                                   hidden_units=gen_hidden_size,\n",
    "                                   output_units=np.prod(image_size))\n",
    "gen_model.build(input_shape=(None, z_size))\n",
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27db12e4-5b0d-4a74-886a-1c34b6078e99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 100)               78500     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 100)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 78,601\n",
      "Trainable params: 78,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc_model = make_discriminator_network(hidden_layers=disc_hidden_layers,\n",
    "                                        hidden_units=disc_hidden_size)\n",
    "disc_model.build(input_shape=(None, np.prod(image_size)))\n",
    "disc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "508ba770-585b-46e2-9511-2400f09bcd52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mnist_bldr = tfds.builder('mnist')\n",
    "mnist_bldr.download_and_prepare()\n",
    "mnist = mnist_bldr.as_dataset(shuffle_files=False)\n",
    "\n",
    "def preprocess(ex, mode='uniform'):\n",
    "    assert mode in ('uniform', 'normal')\n",
    "    image = ex['image']                                       # 0.0-255.0\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)   # 0.0-1.0\n",
    "    image = tf.reshape(image, [-1])                           # flatten\n",
    "    image = image * 2 - 1.0                                   # -1.0-1.0\n",
    "    if mode == 'uniform':\n",
    "        input_z = tf.random.uniform(shape=(z_size,), minval=-1.0, maxval=1.0)\n",
    "    else:   # mode == 'normal'\n",
    "        input_z = tf.random.normal(shape=(z_size,))\n",
    "\n",
    "    return input_z, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32105d2a-81b3-46c6-8a47-721d9b01056d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_z.shape: (32, 20)\n",
      "input_real.shape: (32, 784)\n",
      "g_output.shape: (32, 784)\n",
      "d_logist_real.shape: (32, 1)\n",
      "d_logist_fake.shape: (32, 1)\n",
      "tf.Tensor(\n",
      "[[ 0.07255906]\n",
      " [ 0.6032193 ]\n",
      " [ 0.21647859]\n",
      " [ 0.27428222]\n",
      " [ 0.8696209 ]\n",
      " [ 0.45024568]\n",
      " [ 0.5359401 ]\n",
      " [ 0.86959165]\n",
      " [ 0.4507677 ]\n",
      " [-0.29162532]\n",
      " [ 0.27120787]\n",
      " [ 0.38178104]\n",
      " [ 0.8137118 ]\n",
      " [ 0.29135898]\n",
      " [ 0.20903385]\n",
      " [ 0.39721602]\n",
      " [-0.12903944]\n",
      " [ 0.55011773]\n",
      " [ 0.69221824]\n",
      " [ 0.9343266 ]\n",
      " [ 0.71590745]\n",
      " [ 0.5829706 ]\n",
      " [-0.2631213 ]\n",
      " [ 0.0783605 ]\n",
      " [ 0.79585797]\n",
      " [ 0.5529344 ]\n",
      " [-0.1979656 ]\n",
      " [-0.08673513]\n",
      " [ 0.46586826]\n",
      " [ 0.49324766]\n",
      " [ 0.8305299 ]\n",
      " [ 0.50397414]], shape=(32, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.11402284]\n",
      " [-0.20345762]\n",
      " [-0.18124537]\n",
      " [ 0.05794936]\n",
      " [-0.05845938]\n",
      " [ 0.07330941]\n",
      " [-0.24241251]\n",
      " [-0.18452373]\n",
      " [ 0.09481211]\n",
      " [-0.03855374]\n",
      " [-0.0768047 ]\n",
      " [-0.04407243]\n",
      " [-0.16574474]\n",
      " [-0.10423222]\n",
      " [-0.12113376]\n",
      " [-0.29944634]\n",
      " [-0.01906119]\n",
      " [-0.18081096]\n",
      " [-0.03058762]\n",
      " [-0.34934577]\n",
      " [-0.00152941]\n",
      " [ 0.13642542]\n",
      " [-0.31665003]\n",
      " [-0.20442377]\n",
      " [-0.18440616]\n",
      " [ 0.01118347]\n",
      " [-0.24144273]\n",
      " [ 0.12154298]\n",
      " [-0.13688566]\n",
      " [-0.10542444]\n",
      " [-0.11276428]\n",
      " [ 0.14115204]], shape=(32, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mnist_trainset = mnist['train']\n",
    "mnist_trainset = mnist_trainset.map(preprocess)\n",
    "mnist_trainset = mnist_trainset.batch(32, drop_remainder=True)\n",
    "input_z, input_real = next(iter(mnist_trainset))\n",
    "print('input_z.shape:', input_z.shape)\n",
    "print('input_real.shape:', input_real.shape)\n",
    "\n",
    "g_output = gen_model(input_z)\n",
    "print('g_output.shape:', g_output.shape)\n",
    "\n",
    "d_logits_real = disc_model(input_real)\n",
    "d_logits_fake = disc_model(g_output)\n",
    "print('d_logist_real.shape:', d_logits_real.shape)\n",
    "print('d_logist_fake.shape:', d_logits_fake.shape)\n",
    "\n",
    "print(d_logits_real)\n",
    "print(d_logits_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c1f6902-c426-4538-9af9-a4c987e58914",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 0.7445603\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "g_labels_real = tf.ones_like(d_logits_fake)\n",
    "g_loss = loss_fn(y_true=g_labels_real, y_pred=d_logits_fake)\n",
    "print('Generator loss:', g_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43d8c333-aab4-4973-8bb1-9985fd1457b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator real loss: 0.5252441\n",
      "Discriminator fake loss: 0.648277\n"
     ]
    }
   ],
   "source": [
    "d_labels_real = tf.ones_like(d_logits_real)\n",
    "d_labels_fake = tf.zeros_like(d_logits_fake)\n",
    "d_loss_real = loss_fn(y_true=d_labels_real, y_pred=d_logits_real)\n",
    "d_loss_fake = loss_fn(y_true=d_labels_fake, y_pred=d_logits_fake)\n",
    "\n",
    "print('Discriminator real loss:', d_loss_real.numpy())\n",
    "print('Discriminator fake loss:', d_loss_fake.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "329af646-4a66-4a5e-9f52-4afa0f04572d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "np.random.seed(1)\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "image_size = (28, 28)\n",
    "z_size = 20\n",
    "mode_z = 'uniform'\n",
    "gen_hidden_layers = 1\n",
    "gen_hidden_size = 100\n",
    "disc_hidden_layers = 1\n",
    "disc_hidden_size = 100\n",
    "\n",
    "if mode_z == 'uniform':\n",
    "    fixed_z = tf.random.uniform(shape=(batch_size, z_size),\n",
    "                                minval=-1.0,\n",
    "                                maxval=1.0)\n",
    "elif mode_z == 'normal':\n",
    "    fixed_z = tf.random.normal(shape=(batch_size, z_size))\n",
    "else:\n",
    "    assert False\n",
    "    \n",
    "mnist_trainset = mnist['train']\n",
    "mnist_trainset = mnist_trainset.map(lambda ex: preprocess(ex, mode=mode_z))\n",
    "mnist_trainset = mnist_trainset.shuffle(10000)\n",
    "mnist_trainset = mnist_trainset.batch(32, drop_remainder=True)\n",
    "\n",
    "gen_model = make_generator_network(hidden_layers=gen_hidden_layers,\n",
    "                                   hidden_units=gen_hidden_size,\n",
    "                                   output_units=np.prod(image_size))\n",
    "gen_model.build(input_shape=(None, z_size))\n",
    "\n",
    "disc_model = make_discriminator_network(hidden_layers=disc_hidden_layers,\n",
    "                                        hidden_units=disc_hidden_size)\n",
    "disc_model.build(input_shape=(None, np.prod(image_size)))\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "g_optimizer = tf.keras.optimizers.Adam()\n",
    "d_optimizer = tf.keras.optimizers.Adam()\n",
    "all_losses, all_d_vals, epoch_samples = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecf978-831e-411a-ae76-c4e525a1cd43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | ET 31.74 mins | Mean loss >> G/D 3.5396/0.3215 [D-Real: 0.0756 D-Fake: 0.2458]\n",
      "Epoch: 002 | ET 33.11 mins | Mean loss >> G/D 2.5328/0.8208 [D-Real: 0.3882 D-Fake: 0.4326]\n",
      "Epoch: 003 | ET 35.48 mins | Mean loss >> G/D 1.7863/0.9099 [D-Real: 0.4932 D-Fake: 0.4167]\n",
      "Epoch: 004 | ET 36.85 mins | Mean loss >> G/D 1.3742/1.0810 [D-Real: 0.5738 D-Fake: 0.5072]\n",
      "Epoch: 005 | ET 39.22 mins | Mean loss >> G/D 1.2738/1.0939 [D-Real: 0.5872 D-Fake: 0.5066]\n",
      "Epoch: 006 | ET 40.59 mins | Mean loss >> G/D 1.2017/1.1643 [D-Real: 0.6094 D-Fake: 0.5549]\n",
      "Epoch: 007 | ET 41.90 mins | Mean loss >> G/D 1.1541/1.1808 [D-Real: 0.6145 D-Fake: 0.5663]\n",
      "Epoch: 008 | ET 43.27 mins | Mean loss >> G/D 1.0585/1.2305 [D-Real: 0.6353 D-Fake: 0.5951]\n",
      "Epoch: 009 | ET 44.64 mins | Mean loss >> G/D 1.0152/1.2388 [D-Real: 0.6372 D-Fake: 0.6016]\n",
      "Epoch: 010 | ET 46.03 mins | Mean loss >> G/D 1.0364/1.2433 [D-Real: 0.6362 D-Fake: 0.6071]\n",
      "Epoch: 011 | ET 47.46 mins | Mean loss >> G/D 0.9644/1.2860 [D-Real: 0.6522 D-Fake: 0.6338]\n",
      "Epoch: 012 | ET 49.83 mins | Mean loss >> G/D 0.9934/1.2685 [D-Real: 0.6424 D-Fake: 0.6261]\n",
      "Epoch: 013 | ET 52.19 mins | Mean loss >> G/D 0.9608/1.2887 [D-Real: 0.6524 D-Fake: 0.6363]\n",
      "Epoch: 014 | ET 54.56 mins | Mean loss >> G/D 0.9200/1.3006 [D-Real: 0.6569 D-Fake: 0.6437]\n",
      "Epoch: 015 | ET 56.93 mins | Mean loss >> G/D 0.9818/1.2833 [D-Real: 0.6455 D-Fake: 0.6378]\n",
      "Epoch: 016 | ET 59.30 mins | Mean loss >> G/D 0.9246/1.2830 [D-Real: 0.6492 D-Fake: 0.6338]\n",
      "Epoch: 017 | ET 60.94 mins | Mean loss >> G/D 0.9474/1.2818 [D-Real: 0.6461 D-Fake: 0.6357]\n",
      "Epoch: 018 | ET 62.89 mins | Mean loss >> G/D 0.9059/1.3032 [D-Real: 0.6557 D-Fake: 0.6475]\n",
      "Epoch: 019 | ET 65.26 mins | Mean loss >> G/D 0.9255/1.2927 [D-Real: 0.6505 D-Fake: 0.6422]\n",
      "Epoch: 020 | ET 67.63 mins | Mean loss >> G/D 0.9065/1.3127 [D-Real: 0.6595 D-Fake: 0.6532]\n",
      "Epoch: 021 | ET 69.35 mins | Mean loss >> G/D 0.8964/1.3078 [D-Real: 0.6576 D-Fake: 0.6502]\n",
      "Epoch: 022 | ET 71.04 mins | Mean loss >> G/D 0.9209/1.2966 [D-Real: 0.6515 D-Fake: 0.6451]\n",
      "Epoch: 023 | ET 72.76 mins | Mean loss >> G/D 0.9123/1.3087 [D-Real: 0.6563 D-Fake: 0.6524]\n",
      "Epoch: 024 | ET 74.74 mins | Mean loss >> G/D 0.8883/1.3246 [D-Real: 0.6639 D-Fake: 0.6607]\n",
      "Epoch: 025 | ET 76.65 mins | Mean loss >> G/D 0.9023/1.3195 [D-Real: 0.6604 D-Fake: 0.6590]\n",
      "Epoch: 026 | ET 78.53 mins | Mean loss >> G/D 0.9105/1.3160 [D-Real: 0.6594 D-Fake: 0.6566]\n",
      "Epoch: 027 | ET 80.44 mins | Mean loss >> G/D 0.8684/1.3340 [D-Real: 0.6667 D-Fake: 0.6673]\n",
      "Epoch: 028 | ET 82.81 mins | Mean loss >> G/D 0.8952/1.3181 [D-Real: 0.6605 D-Fake: 0.6575]\n",
      "Epoch: 029 | ET 85.18 mins | Mean loss >> G/D 0.8909/1.3295 [D-Real: 0.6662 D-Fake: 0.6633]\n",
      "Epoch: 030 | ET 87.09 mins | Mean loss >> G/D 0.8688/1.3304 [D-Real: 0.6680 D-Fake: 0.6623]\n",
      "Epoch: 031 | ET 89.46 mins | Mean loss >> G/D 0.8978/1.3297 [D-Real: 0.6645 D-Fake: 0.6652]\n",
      "Epoch: 032 | ET 91.83 mins | Mean loss >> G/D 0.8626/1.3302 [D-Real: 0.6674 D-Fake: 0.6628]\n",
      "Epoch: 033 | ET 93.86 mins | Mean loss >> G/D 0.9147/1.3200 [D-Real: 0.6615 D-Fake: 0.6585]\n",
      "Epoch: 034 | ET 96.23 mins | Mean loss >> G/D 0.8699/1.3273 [D-Real: 0.6656 D-Fake: 0.6617]\n",
      "Epoch: 035 | ET 98.60 mins | Mean loss >> G/D 0.8858/1.3264 [D-Real: 0.6617 D-Fake: 0.6648]\n",
      "Epoch: 036 | ET 100.97 mins | Mean loss >> G/D 0.8899/1.3232 [D-Real: 0.6634 D-Fake: 0.6597]\n",
      "Epoch: 037 | ET 103.34 mins | Mean loss >> G/D 0.8897/1.3282 [D-Real: 0.6646 D-Fake: 0.6636]\n",
      "Epoch: 038 | ET 105.71 mins | Mean loss >> G/D 0.8718/1.3216 [D-Real: 0.6635 D-Fake: 0.6581]\n",
      "Epoch: 039 | ET 108.14 mins | Mean loss >> G/D 0.8958/1.3260 [D-Real: 0.6637 D-Fake: 0.6623]\n",
      "Epoch: 040 | ET 110.32 mins | Mean loss >> G/D 0.8819/1.3265 [D-Real: 0.6642 D-Fake: 0.6623]\n",
      "Epoch: 041 | ET 112.69 mins | Mean loss >> G/D 0.8755/1.3281 [D-Real: 0.6643 D-Fake: 0.6638]\n",
      "Epoch: 042 | ET 115.06 mins | Mean loss >> G/D 0.8839/1.3307 [D-Real: 0.6666 D-Fake: 0.6641]\n",
      "Epoch: 043 | ET 117.43 mins | Mean loss >> G/D 0.9006/1.3322 [D-Real: 0.6656 D-Fake: 0.6667]\n",
      "Epoch: 044 | ET 119.80 mins | Mean loss >> G/D 0.8757/1.3369 [D-Real: 0.6709 D-Fake: 0.6659]\n",
      "Epoch: 045 | ET 122.17 mins | Mean loss >> G/D 0.8967/1.3275 [D-Real: 0.6652 D-Fake: 0.6622]\n",
      "Epoch: 046 | ET 125.54 mins | Mean loss >> G/D 0.8688/1.3413 [D-Real: 0.6728 D-Fake: 0.6685]\n",
      "Epoch: 047 | ET 128.91 mins | Mean loss >> G/D 0.8742/1.3368 [D-Real: 0.6676 D-Fake: 0.6692]\n",
      "Epoch: 048 | ET 131.34 mins | Mean loss >> G/D 0.8866/1.3333 [D-Real: 0.6667 D-Fake: 0.6665]\n",
      "Epoch: 049 | ET 133.77 mins | Mean loss >> G/D 0.8771/1.3370 [D-Real: 0.6693 D-Fake: 0.6677]\n",
      "Epoch: 050 | ET 137.14 mins | Mean loss >> G/D 0.8666/1.3446 [D-Real: 0.6732 D-Fake: 0.6715]\n",
      "Epoch: 051 | ET 140.51 mins | Mean loss >> G/D 0.8915/1.3314 [D-Real: 0.6662 D-Fake: 0.6652]\n",
      "Epoch: 052 | ET 143.00 mins | Mean loss >> G/D 0.8797/1.3359 [D-Real: 0.6690 D-Fake: 0.6669]\n",
      "Epoch: 053 | ET 146.37 mins | Mean loss >> G/D 0.8757/1.3403 [D-Real: 0.6700 D-Fake: 0.6703]\n",
      "Epoch: 054 | ET 149.74 mins | Mean loss >> G/D 0.8808/1.3344 [D-Real: 0.6679 D-Fake: 0.6665]\n",
      "Epoch: 055 | ET 153.11 mins | Mean loss >> G/D 0.8739/1.3364 [D-Real: 0.6681 D-Fake: 0.6683]\n",
      "Epoch: 056 | ET 156.48 mins | Mean loss >> G/D 0.8833/1.3296 [D-Real: 0.6661 D-Fake: 0.6635]\n",
      "Epoch: 057 | ET 159.86 mins | Mean loss >> G/D 0.8982/1.3236 [D-Real: 0.6629 D-Fake: 0.6607]\n",
      "Epoch: 058 | ET 163.23 mins | Mean loss >> G/D 0.8857/1.3363 [D-Real: 0.6694 D-Fake: 0.6669]\n",
      "Epoch: 059 | ET 166.60 mins | Mean loss >> G/D 0.8705/1.3435 [D-Real: 0.6721 D-Fake: 0.6714]\n",
      "Epoch: 060 | ET 169.97 mins | Mean loss >> G/D 0.8862/1.3370 [D-Real: 0.6695 D-Fake: 0.6675]\n",
      "Epoch: 061 | ET 172.80 mins | Mean loss >> G/D 0.8813/1.3262 [D-Real: 0.6649 D-Fake: 0.6612]\n",
      "Epoch: 062 | ET 176.17 mins | Mean loss >> G/D 0.8754/1.3393 [D-Real: 0.6692 D-Fake: 0.6700]\n",
      "Epoch: 063 | ET 179.54 mins | Mean loss >> G/D 0.8762/1.3381 [D-Real: 0.6708 D-Fake: 0.6673]\n",
      "Epoch: 064 | ET 182.91 mins | Mean loss >> G/D 0.8664/1.3448 [D-Real: 0.6730 D-Fake: 0.6718]\n",
      "Epoch: 065 | ET 185.86 mins | Mean loss >> G/D 0.8875/1.3385 [D-Real: 0.6696 D-Fake: 0.6689]\n",
      "Epoch: 066 | ET 189.23 mins | Mean loss >> G/D 0.8717/1.3383 [D-Real: 0.6696 D-Fake: 0.6686]\n",
      "Epoch: 067 | ET 192.25 mins | Mean loss >> G/D 0.8885/1.3399 [D-Real: 0.6691 D-Fake: 0.6708]\n",
      "Epoch: 068 | ET 195.29 mins | Mean loss >> G/D 0.8816/1.3356 [D-Real: 0.6670 D-Fake: 0.6687]\n",
      "Epoch: 069 | ET 198.66 mins | Mean loss >> G/D 0.8803/1.3346 [D-Real: 0.6676 D-Fake: 0.6669]\n",
      "Epoch: 070 | ET 202.04 mins | Mean loss >> G/D 0.8862/1.3401 [D-Real: 0.6698 D-Fake: 0.6703]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    epoch_losses, epoch_d_vals = [], []\n",
    "    for i, (input_z, input_real) in enumerate(mnist_trainset):\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            g_output = gen_model(input_z)\n",
    "            d_logits_fake = disc_model(g_output, training=True)\n",
    "            labels_real = tf.ones_like(d_logits_fake)\n",
    "            g_loss = loss_fn(y_true=labels_real, y_pred=d_logits_fake)\n",
    "            \n",
    "        g_grads = g_tape.gradient(g_loss, gen_model.trainable_variables)\n",
    "        g_optimizer.apply_gradients(grads_and_vars=zip(g_grads,\n",
    "                                                       gen_model.trainable_variables))\n",
    "        \n",
    "        with tf.GradientTape() as d_tape:\n",
    "            d_logits_real = disc_model(input_real, training=True)\n",
    "            d_labels_real = tf.ones_like(d_logits_real)\n",
    "            d_loss_real = loss_fn(y_true=d_labels_real, y_pred=d_logits_real)\n",
    "            \n",
    "            d_logits_fake = disc_model(g_output, training=True)\n",
    "            d_labels_fake = tf.zeros_like(d_logits_fake)\n",
    "            d_loss_fake = loss_fn(y_true=d_labels_fake, y_pred=d_logits_fake)\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            \n",
    "        d_grads = d_tape.gradient(d_loss, disc_model.trainable_variables)\n",
    "        d_optimizer.apply_gradients(grads_and_vars=zip(d_grads,\n",
    "                                                       disc_model.trainable_variables))\n",
    "\n",
    "        epoch_losses.append((g_loss.numpy(),\n",
    "                             d_loss.numpy(),\n",
    "                             d_loss_real.numpy(),\n",
    "                             d_loss_fake.numpy()))\n",
    "        d_probs_real = tf.reduce_mean(tf.sigmoid(d_logits_real))\n",
    "        d_probs_fake = tf.reduce_mean(tf.sigmoid(d_logits_fake))\n",
    "        epoch_d_vals.append((d_probs_real.numpy(), d_probs_fake.numpy()))\n",
    "        \n",
    "    all_losses.append(epoch_losses)\n",
    "    all_d_vals.append(epoch_d_vals)\n",
    "    \n",
    "    print(\n",
    "        'Epoch: {:03d} | ET {:.2f} mins | Mean loss >>'\n",
    "        ' G/D {:.4f}/{:.4f} [D-Real: {:.4f} D-Fake: {:.4f}]'.format(\n",
    "            epoch,\n",
    "            (time.time() - start_time) / 60,\n",
    "            *list(np.mean(all_losses[-1], axis=0))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a05120eb-15ac-4007-8128-a8296b605641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: 0.699999988079071 / var2: 1.600000023841858\n",
      "var1: 0.4899999797344208 / var2: 1.2799999713897705\n",
      "var1: 0.34299999475479126 / var2: 1.0239999294281006\n",
      "var1: 0.2400999814271927 / var2: 0.8191999197006226\n",
      "var1: 0.168069988489151 / var2: 0.6553599238395691\n",
      "var1: 0.11764898896217346 / var2: 0.5242879390716553\n",
      "var1: 0.08235429227352142 / var2: 0.41943034529685974\n",
      "var1: 0.05764800310134888 / var2: 0.33554428815841675\n",
      "var1: 0.04035360366106033 / var2: 0.26843541860580444\n",
      "var1: 0.028247522190213203 / var2: 0.2147483378648758\n",
      "var1: 0.019773265346884727 / var2: 0.1717986762523651\n",
      "var1: 0.013841286301612854 / var2: 0.13743893802165985\n",
      "var1: 0.009688900783658028 / var2: 0.10995115339756012\n",
      "var1: 0.006782230455428362 / var2: 0.08796092122793198\n",
      "var1: 0.004747561179101467 / var2: 0.07036873698234558\n",
      "var1: 0.003323292825371027 / var2: 0.056294988840818405\n",
      "var1: 0.0023263050243258476 / var2: 0.045035991817712784\n",
      "var1: 0.001628413563594222 / var2: 0.036028794944286346\n",
      "var1: 0.001139889471232891 / var2: 0.028823036700487137\n",
      "var1: 0.0007979226065799594 / var2: 0.02305842936038971\n",
      "var1: 0.0005585458129644394 / var2: 0.018446743488311768\n",
      "var1: 0.00039098208071663976 / var2: 0.014757394790649414\n",
      "var1: 0.00027368744486011565 / var2: 0.011805916205048561\n",
      "var1: 0.00019158120267093182 / var2: 0.009444733150303364\n",
      "var1: 0.00013410684186965227 / var2: 0.007555786520242691\n",
      "var1: 9.387479076394811e-05 / var2: 0.006044629029929638\n",
      "var1: 6.571235280716792e-05 / var2: 0.004835703410208225\n",
      "var1: 4.5998647692613304e-05 / var2: 0.0038685626350343227\n",
      "var1: 3.219905192963779e-05 / var2: 0.003094850108027458\n",
      "var1: 2.2539335986948572e-05 / var2: 0.0024758800864219666\n",
      "var1: 1.577753573656082e-05 / var2: 0.0019807040225714445\n",
      "var1: 1.1044275197491515e-05 / var2: 0.0015845631714910269\n",
      "var1: 7.73099236539565e-06 / var2: 0.0012676505139097571\n",
      "var1: 5.411694473878015e-06 / var2: 0.0010141204111278057\n",
      "var1: 3.78818594981567e-06 / var2: 0.0008112963405437768\n",
      "var1: 2.6517300284467638e-06 / var2: 0.0006490370724350214\n",
      "var1: 1.8562110426501022e-06 / var2: 0.0005192296812310815\n",
      "var1: 1.2993476730116527e-06 / var2: 0.0004153837508056313\n",
      "var1: 9.095433597394731e-07 / var2: 0.0003323070122860372\n",
      "var1: 6.366803404489474e-07 / var2: 0.00026584562147036195\n",
      "var1: 4.4567624968294695e-07 / var2: 0.00021267650299705565\n",
      "var1: 3.1197336625155003e-07 / var2: 0.00017014119657687843\n",
      "var1: 2.1838135921825597e-07 / var2: 0.0001361129543511197\n",
      "var1: 1.52866959979292e-07 / var2: 0.0001088903663912788\n",
      "var1: 1.0700686914333346e-07 / var2: 8.711229020264e-05\n",
      "var1: 7.490481124250437e-08 / var2: 6.968983507249504e-05\n",
      "var1: 5.2433367869753056e-08 / var2: 5.5751868785591796e-05\n",
      "var1: 3.670335857464124e-08 / var2: 4.460149648366496e-05\n",
      "var1: 2.5692351357520238e-08 / var2: 3.568119791452773e-05\n",
      "var1: 1.7984646660806902e-08 / var2: 2.8544958695420064e-05\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "var1, var2 = tf.Variable(1.0), tf.Variable(2.0)\n",
    "loss = lambda: 3 * var1 * var1 + 2 * var2 * var2\n",
    "for i in range(50):\n",
    "    opt.minimize(loss, var_list=[var1, var2])\n",
    "    print(f'var1: {var1.numpy()} / var2: {var2.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f01b000e-06f3-4422-ae99-906e084ddca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1: 0.8999970555305481 / var2: 1.8999969959259033\n",
      "var1: 0.8004081845283508 / var2: 1.8001623153686523\n",
      "var1: 0.7015819549560547 / var2: 1.700618863105774\n",
      "var1: 0.6039344072341919 / var2: 1.6015000343322754\n",
      "var1: 0.5079587697982788 / var2: 1.5029505491256714\n",
      "var1: 0.414231538772583 / var2: 1.4051263332366943\n",
      "var1: 0.3234156668186188 / var2: 1.3081938028335571\n",
      "var1: 0.23625895380973816 / var2: 1.2123314142227173\n",
      "var1: 0.1535801738500595 / var2: 1.1177276372909546\n",
      "var1: 0.07624486833810806 / var2: 1.0245814323425293\n",
      "var1: 0.005127407610416412 / var2: 0.9331026077270508\n",
      "var1: -0.058941639959812164 / var2: 0.8435097336769104\n",
      "var1: -0.11523428559303284 / var2: 0.7560288906097412\n",
      "var1: -0.163182333111763 / var2: 0.6708924174308777\n",
      "var1: -0.2024233639240265 / var2: 0.5883365273475647\n",
      "var1: -0.23282656073570251 / var2: 0.5085986852645874\n",
      "var1: -0.25449511408805847 / var2: 0.43191468715667725\n",
      "var1: -0.267747700214386 / var2: 0.35851553082466125\n",
      "var1: -0.2730855941772461 / var2: 0.2886234521865845\n",
      "var1: -0.27115336060523987 / var2: 0.22244775295257568\n",
      "var1: -0.26269978284835815 / var2: 0.1601814329624176\n",
      "var1: -0.24854356050491333 / var2: 0.10199644416570663\n",
      "var1: -0.22954536974430084 / var2: 0.048040084540843964\n",
      "var1: -0.20658636093139648 / var2: -0.001568380743265152\n",
      "var1: -0.18055161833763123 / var2: -0.04674087092280388\n",
      "var1: -0.15231721103191376 / var2: -0.08742263913154602\n",
      "var1: -0.12273889780044556 / var2: -0.12359398603439331\n",
      "var1: -0.09264139831066132 / var2: -0.15527091920375824\n",
      "var1: -0.06280722469091415 / var2: -0.1825050562620163\n",
      "var1: -0.03396456688642502 / var2: -0.20538286864757538\n",
      "var1: -0.006774883717298508 / var2: -0.22402386367321014\n",
      "var1: 0.018180064857006073 / var2: -0.23857860267162323\n",
      "var1: 0.04040990769863129 / var2: -0.24922583997249603\n",
      "var1: 0.05952564999461174 / var2: -0.25616949796676636\n",
      "var1: 0.0752466470003128 / var2: -0.2596352696418762\n",
      "var1: 0.08740394562482834 / var2: -0.25986722111701965\n",
      "var1: 0.09593965113162994 / var2: -0.2571241557598114\n",
      "var1: 0.10090233385562897 / var2: -0.2516762614250183\n",
      "var1: 0.10243900120258331 / var2: -0.24380163848400116\n",
      "var1: 0.10078424960374832 / var2: -0.23378315567970276\n",
      "var1: 0.09624741971492767 / var2: -0.22190538048744202\n",
      "var1: 0.08919848501682281 / var2: -0.20845165848731995\n",
      "var1: 0.08005324751138687 / var2: -0.19370147585868835\n",
      "var1: 0.0692584440112114 / var2: -0.17792801558971405\n",
      "var1: 0.05727699398994446 / var2: -0.16139580309391022\n",
      "var1: 0.04457380250096321 / var2: -0.14435866475105286\n",
      "var1: 0.031602177768945694 / var2: -0.12705785036087036\n",
      "var1: 0.018791256472468376 / var2: -0.10972025245428085\n",
      "var1: 0.00653460156172514 / var2: -0.09255700558423996\n",
      "var1: -0.004819740541279316 / var2: -0.07576215267181396\n",
      "var1: -0.014977438375353813 / var2: -0.05951154977083206\n",
      "var1: -0.02370402216911316 / var2: -0.04396204650402069\n",
      "var1: -0.030828844755887985 / var2: -0.029250841587781906\n",
      "var1: -0.03624671697616577 / var2: -0.015495076775550842\n",
      "var1: -0.03991718962788582 / var2: -0.0027917250990867615\n",
      "var1: -0.04186168685555458 / var2: 0.008782361634075642\n",
      "var1: -0.04215860366821289 / var2: 0.019170109182596207\n",
      "var1: -0.04093676432967186 / var2: 0.028333693742752075\n",
      "var1: -0.038367509841918945 / var2: 0.03625378757715225\n",
      "var1: -0.034655846655368805 / var2: 0.04292858764529228\n",
      "var1: -0.0300309956073761 / var2: 0.04837271198630333\n",
      "var1: -0.024736696854233742 / var2: 0.052615921944379807\n",
      "var1: -0.019021648913621902 / var2: 0.055701710283756256\n",
      "var1: -0.013130336999893188 / var2: 0.05768580734729767\n",
      "var1: -0.0072946129366755486 / var2: 0.058634597808122635\n",
      "var1: -0.0017261914908885956 / var2: 0.05862349271774292\n",
      "var1: 0.0033896462991833687 / var2: 0.05773529037833214\n",
      "var1: 0.007899021729826927 / var2: 0.056058526039123535\n",
      "var1: 0.011682946234941483 / var2: 0.05368584766983986\n",
      "var1: 0.01465929951518774 / var2: 0.050712455064058304\n",
      "var1: 0.01678328588604927 / var2: 0.047234587371349335\n",
      "var1: 0.018046434968709946 / var2: 0.04334809258580208\n",
      "var1: 0.018474269658327103 / var2: 0.03914710506796837\n",
      "var1: 0.018122825771570206 / var2: 0.03472282737493515\n",
      "var1: 0.01707420125603676 / var2: 0.03016243875026703\n",
      "var1: 0.015431404113769531 / var2: 0.025548111647367477\n",
      "var1: 0.013312725350260735 / var2: 0.020956192165613174\n",
      "var1: 0.010845893993973732 / var2: 0.016456499695777893\n",
      "var1: 0.008162261918187141 / var2: 0.01211177371442318\n",
      "var1: 0.005391242913901806 / var2: 0.007977260276675224\n",
      "var1: 0.0026552071794867516 / var2: 0.004100433550775051\n",
      "var1: 6.50240108370781e-05 / var2: 0.0005208610091358423\n",
      "var1: -0.0022836255375295877 / var2: -0.0027298121713101864\n",
      "var1: -0.004313052166253328 / var2: -0.005627756007015705\n",
      "var1: -0.005965419113636017 / var2: -0.008156731724739075\n",
      "var1: -0.007203641813248396 / var2: -0.010307768359780312\n",
      "var1: -0.008011355996131897 / var2: -0.012078754603862762\n",
      "var1: -0.008392003364861012 / var2: -0.013473939150571823\n",
      "var1: -0.00836714543402195 / var2: -0.014503377489745617\n",
      "var1: -0.00797412358224392 / var2: -0.015182319097220898\n",
      "var1: -0.00726320268586278 / var2: -0.015530559234321117\n",
      "var1: -0.006294368766248226 / var2: -0.015571759082376957\n",
      "var1: -0.005133934784680605 / var2: -0.015332754701375961\n",
      "var1: -0.0038511198945343494 / var2: -0.01484286691993475\n",
      "var1: -0.0025147502310574055 / var2: -0.0141332121565938\n",
      "var1: -0.001190222566947341 / var2: -0.01323604304343462\n",
      "var1: 6.315566133707762e-05 / var2: -0.012184112332761288\n",
      "var1: 0.0011943564750254154 / var2: -0.011010076850652695\n",
      "var1: 0.002162280958145857 / var2: -0.00974594708532095\n",
      "var1: 0.0029368409886956215 / var2: -0.008422587998211384\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "var1, var2 = tf.Variable(1.0), tf.Variable(2.0)\n",
    "loss = lambda: 3 * var1 * var1 + 2 * var2 * var2\n",
    "for i in range(100):\n",
    "    opt.minimize(loss, var_list=[var1, var2])\n",
    "    print(f'var1: {var1.numpy()} / var2: {var2.numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_conda_env)",
   "language": "python",
   "name": "tf_conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
